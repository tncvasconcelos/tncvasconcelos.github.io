basedir = "~/Desktop/MiSSEgradient/MiSSEGradient/full_run"
taxonomy_tree = phy.list(input.dir=paste0(basedir,"/Data"), names="ALLMB.taxized.tre")[[1]]
Resolve <- function(phy, prune_unmatched=TRUE, delete_author=FALSE) {
sources <- taxize::gnr_datasources()
gbif <- sources$id[sources$title == 'GBIF Backbone Taxonomy']
gnr_resolve_x <- function(x, data_source_ids=gbif, best_match_only = TRUE) {
new.name <- suppressWarnings(taxize::gnr_resolve(names=x, data_source_ids=data_source_ids, best_match_only=best_match_only)$matched_name)
if(is.null(new.name)) {
new.name <- paste0("UNMATCHED_",x)
}
return(new.name)
}
new.names <- pbapply::pbsapply(phy$tip.label, gnr_resolve_x)
if(length(new.names) != length(phy$tip.label)) {
stop("Fewer names returned")
}
phy$tip.label <- new.names
if(prune_unmatched) {
bad.tips <- phy$tip.label[grepl("UNMATCHED_", phy$tip.label)]
if(length(bad.tips)>0) {
phy <- ape::drop.tip(phy, bad.tips)
}
}
if(delete_author) {
tip_name <- strsplit(phy$tip.label," ",fixed = TRUE)
phy$tip.label <- paste(sapply(tip_name, "[", 1), sapply(tip_name, "[", 2), sep="_")
}
return(phy)
}
GetSubTree <- function(clade.name, full_phy){
node.number <- which(full_phy$node.label == clade.name) + Ntip(full_phy)
all.tips <- Descendants(full_phy, node.number, type = c("tips"))[[1]]
set <- full_phy$tip.label
rows <- which(set %in% full_phy$tip.label[all.tips])
byebye <- set[-rows]
subTree <- drop.tip(full_phy, as.character(byebye))
return(subTree)
}
CleanGBIFRecords <- function(gbif.file, basedir) {
#Basic idea, is to do everything as we search by taxon:
subset.mat <- gbif.file
#Step 1: Parse out duplicates with respect to lat and long:
subset.mat <- subset.mat[!duplicated(subset.mat[,3]) & !duplicated(subset.mat[,4]),]
#Step 2: Remove human observations from the list.
subset.mat <- subset.mat[which(!subset.mat[,5]=="HUMAN_OBSERVATION" & !subset.mat[,5]=="UNKNOWN" & !subset.mat[,5]=="LIVING_SPECIMEN"),]
#Step 3: Remove centroids
subset.mat <- CoordinateCleaner::cc_cen(subset.mat, lon="long", lat="lat", species ="name", geod=T, buffer=25000, verbose=F)
#Step 4: Remove points with no decimal cases
length_decimal_lat <- nchar(sub("^[^.]*", "", subset.mat$lat))-1
length_decimal_lon <- nchar(sub("^[^.]*", "", subset.mat$long))-1
subset.mat <- subset.mat[which(length_decimal_lat>=2 & length_decimal_lon>=2),]
#Step 5: Removing outliers in distribution
unique(subset.mat$name)->species
cleaned_point <- data.frame()
for(cleaning in 1:length(species)){
sp0<-subset.mat[subset.mat$name==species[cleaning],]
out_lat<-boxplot.stats(sp0$lat)$out
out_lon<-boxplot.stats(sp0$long)$out
sp0 <- sp0[ ! sp0$lat %in% out_lat, ]
sp0 <- sp0[ ! sp0$long %in% out_lon, ]
cleaned_point <- rbind(cleaned_point, sp0)
}
subset.mat<-cleaned_point
#Step 6: Remove points that are likely the location of herbaria
#Basically, I first match by lat, then by long.
# herbaria_localities <- read.csv("/home/tvasconcelos/MiSSEGradient/Data/allHerbaria_ADM1_badCoords.txt", header=FALSE)
herbaria_localities <- read.csv(paste0(basedir, "/Data/allHerbaria_ADM1_badCoords.txt"), header=FALSE)
for(herb.index in 1:length(herbaria_localities)){
subset.mat <- subset.mat[which(!round(subset.mat$lat,2)==herbaria_localities[herb.index,1] | !round(subset.mat$long,2)==herbaria_localities[herb.index,2]),]
}
#Step 7: Remove points based on issues. Removed any records with the following codes: "cdout", "cdrepf", etc.
tmp.vector <- c()
if(dim(subset.mat)[1]>0){
issues.list.by.record <- strsplit(as.character(subset.mat$issues), ",")
for(record.index in 1:dim(subset.mat)[1]){
if(any(issues.list.by.record[[record.index]] == "ccm" | issues.list.by.record[[record.index]] == "conti" | issues.list.by.record[[record.index]] == "cdiv" | issues.list.by.record[[record.index]] == "cdout" | issues.list.by.record[[record.index]] == "cdrepf" | issues.list.by.record[[record.index]] == "cdreps" | issues.list.by.record[[record.index]] == "cucdmis" | issues.list.by.record[[record.index]] == "cuiv" | issues.list.by.record[[record.index]] == "cum" |  issues.list.by.record[[record.index]] == "gdativ" | issues.list.by.record[[record.index]] == "reneglat" | issues.list.by.record[[record.index]] == "reneglon" | issues.list.by.record[[record.index]] == "preswcd" | issues.list.by.record[[record.index]] == "zerocd" | issues.list.by.record[[record.index]] == "txmatfuz" | issues.list.by.record[[record.index]] == "txmathi" | issues.list.by.record[[record.index]] == "txmatnon")){
tmp.vector <- tmp.vector
}else{
tmp.vector <- c(tmp.vector, record.index)
}
}
final.mat <- subset.mat[tmp.vector,]
return(final.mat)
}else{
return(NULL)
}
}
GetGbifRecordOneTaxonSurviveFailure <- function(taxon, clade="none", basedir=getwd()){
final.df <- c()
return.taxon <- NA
search.hits <- NA
slow.time <- 60
while(is.na(search.hits)[1]) {
try(search.hits <- occ_search(scientificName = taxon, limit=50000,  hasCoordinate=TRUE)$data)
if(is.null(search.hits)) { return("no_data") }
else if(is.na(search.hits)) {
Sys.sleep(slow.time)
slow.time <- min(slow.time*1.2, 60*15)
}
}
if((search.hits[1] == "no data found, try a different search")[1]){
print("No records")
}else{
tmp.df <- data.frame(name=search.hits$acceptedScientificName, key=search.hits$key, lat=search.hits$decimalLatitude, long=search.hits$decimalLongitude, basisOfRecord=search.hits$basisOfRecord, issues=search.hits$issues)
tmp.df.clean <- try(CleanGBIFRecords(tmp.df, basedir))
if(!is.null(tmp.df.clean) & class(tmp.df.clean)!="try-error") {
#We require that each species has, at a minimum, 10 records:
if(dim(tmp.df.clean)[1]>2){
names(tmp.df.clean) <- c("species", "key", "lat", "long", "basisOfRecord", "issue")
write.table(tmp.df.clean, file=paste0(basedir, "/Data/2_gbif/", clade, "__", gsub(" ", "_", taxon), ".csv"), quote=FALSE, sep="\t", row.names=FALSE, col.names=FALSE, append=FALSE)
return.taxon <- taxon
}
}
}
return(return.taxon)
}
DoOneClade <- function(individual_clade, large_gb_tree, ncores=1, basedir=getwd()) {
subtree <- GetSubTree(unname(individual_clade), large_gb_tree)
ape::write.tree(subtree, file=paste0(basedir,"/Data/1_trees/", names(individual_clade),".tre"))
taxized <- Resolve(subtree)
ape::write.tree(taxized, file=paste0(basedir,"/Data/1_trees/", names(individual_clade), ".taxized.tre"))
gbif_cleaned <- lapply(taxized$tip.label, FUN=GetGbifRecordOneTaxonSurviveFailure, clade=names(individual_clade),  basedir=basedir)
gbif_cleaned <- gbif_cleaned[!is.na(gbif_cleaned)]
gbif_cleaned_prefix <- paste0(paste0(names(individual_clade), "__"), gbif_cleaned)
return(gbif_cleaned_prefix)
}
# Functions to run closesamples #------------
phy.list <- function (input.dir=input.dir, names=names, search.for=".taxized.tre") {
l0 <- list.files(input.dir, search.for)
l1 <- sub(search.for, "", l0)
tree_names <- l0[which(l1 %in% names)]
# Read tree files and .txt
tmp.files1 <- lapply(paste0(input.dir,"/",tree_names), readLines)
# Removing single quotes from authority names
tmp.files2 <- lapply(tmp.files1, function(x) gsub("\\'","",as.character(x)))
tree_list <- list()
for(tree_index in sequence(length(tmp.files2))){
# Read tree files and phylo
t0 <- ape::read.tree(text=tmp.files2[[tree_index]])
t0 <- ape::ladderize(t0)
# Keeping only species names. Note that this will also lump "var." and "subsp." into the same species name.
name <- strsplit(t0$tip.label,"_",fixed = TRUE)
t0$tip.label <- paste(sapply(name, "[", 1), sapply(name, "[", 2), sep=" ")
t0$tip.label <- sub(",","", t0$tip.label)
# Removing node labels
t0$node.label <- NULL
# Removing duplicated tips and tips identified only to genus level
t0 <- ape::drop.tip(t0, which(unlist(lapply(1:length(name), function(x) stringr::str_detect(substr(name[[x]][2], 1, 1), "^[:upper:]+$")))))
t0 <- ape::drop.tip(t0, which(duplicated(t0$tip.label)))
tree_list[[tree_index]] <- t0
names(tree_list)[tree_index] <- sub(search.for, "", tree_names)[tree_index]
}
return(tree_list)
}
run.closesamples <- function(cladenames, phy_full=taxonomy_tree, fraction=0.75, input.dir=input.dir, ncores=4) {
clades0 <- phy.list(input.dir=input.dir, names=unname(cladenames), search.for=".taxized.tre")
clades <- clades0[[1]]
if(ape::Ntip(clades)-1 != ape::Nnode(clades)) { # removing all tips descending from polytomies
d0 <- tabulate(clades$edge[, 1])
polynodes <- which(d0 > 2)
polytips <- phangorn::Descendants(clades, polynodes, type="tips")
clades <- ape::drop.tip(clades, unique(clades$tip.label[unlist(polytips)]))
}
sub0 <-  closesamples::SubsampleTree(phy_feasible=clades, n = round(Ntip(clades)*fraction, 0), phy_full=phy_full, replace_full=FALSE, replace_feasible=FALSE, less_memory=TRUE, truncate_full_to_mrca=TRUE, fast_ultrametric=FALSE, verbose=FALSE)
ape::write.tree(sub0, file=paste0(input.dir, "/sampled_trees/", names(clades0), "_", format(Sys.time(), "%b%d%H%M%S"), sample(1:100, 1),".sampled.tre"))
return(sub0)
}
# Functions to run MiSSE #------------
# closesamples
MiSSEGreedy_fullrun_cs <- function(tree_files = tree_files, basedir, total_diversity=total_diversity) {
tree_file = unname(tree_files)
phy = ape::read.tree(paste0(basedir, "Data/sampled_trees/", tree_file))
tree_name = sapply(strsplit(tree_file, "_"), "[", 1)
total_clade = as.numeric(total_diversity[which(names(total_diversity) == tree_name)])
### parameters
turnover.tries = sequence(10)
eps.tries = sequence(3)
max.param = length(turnover.tries) + length(eps.tries)
# creating a matrix of possible combinations of parameter:
comb_1 = hisse::generateMiSSEGreedyCombinations(max.param = max.param, turnover.tries = turnover.tries,
eps.tries = eps.tries, fixed.eps.tries=c(0, 0.9, NA),
vary.both=T)
# run MiSSE models:
unique_name <- paste0(format(Sys.time(), "%b%d%H%M%S"), sample(1:100, 1))
run_1 = hisse::MiSSEGreedy(phy, f = ape::Ntip(phy)/total_clade, possible.combos = comb_1, save.file=paste0(basedir,"Data/modelfits/", tree_name,"_",unique_name,"_fits_run_1.Rsave"),
root.type="madfitz", stop.deltaAICc=Inf, chunk.size = 10, turnover.upper=100, trans.upper=10, sann=TRUE, sann.its=1000)
model.recons1 <- as.list(1:length(run_1))
for (model_index in 1:length(run_1)) {
nturnover <- length(unique(run_1[[model_index]]$turnover))
neps <- length(unique(run_1[[model_index]]$eps))
hisse_recon <- hisse::MarginReconMiSSE(phy = run_1[[model_index]]$phy, f = Ntip(phy)/total_clade, hidden.states = nturnover,
pars = run_1[[model_index]]$solution, aic = run_1[[model_index]]$AIC, root.type = "madfitz")
model.recons1[[model_index]] <- hisse_recon
}
save(model.recons1, file = paste0(basedir,"Data/modelrecons/", tree_name, "_", unique_name ,"_model_recons1.Rsave"))
return(model.recons1)
}
taxonomy_tree = phy.list(input.dir=paste0(basedir,"/Data"), names="ALLMB.taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/1_trees"), names="ALLMB", search.for=".taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/"), names="ALLMB", search.for=".taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/"), names="GBMB", search.for=".tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for="taxized.tre")[[1]]
basedir
input.dir=paste0(basedir, "/Data")
input.dir
phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for="taxized.tre")
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for=".taxized.tre")
rmarkdown::render_site()
rmarkdown::render_site()
QuickGBIFSearch<- function (species, limit = 100000, remove.na=TRUE) {
datalist = list()
for (z in 1:length(species)){
tt <- strsplit(species," ",fixed = TRUE)[[z]]
tt[1] -> x
tt[2] -> y
# NonT:
cat(paste0(x," ",y)) # checking names
cat("","\n")
if(dismo::gbif(x, y, download = F) > limit){
cat("too many records to download")
} else {
dat <- dismo::gbif(x, y) # downloading points from gbif
datalist[[z]] <- dat
}
}
results <- do.call(rbind, datalist)
results <- results[,c("species","lat","lon")]
if(remove.na){
na_coord <- c(length(which(is.na(results$lat))), length(which(is.na(results$lon))))
cat(max(na_coord), "- records had NA for either lat or lon and were excluded from the final table. Set remove.na=FALSE option to retrieve them")
results <- na.omit(results)
}
#
return(results)
}
QuickGBIFCleaning <- function(points, type=c("full", ...), ccbuffer=25000) {
# reading files
if(ncol(points) > 3) {
warning("Data.frame seems to have more than three columns. Make sure that it is formatted as: first column - species names, second column - latitude, third columns - longitude.")
}
if(!inherits(points, "data.frame")) {
stop("Input object is not a data.frame.")
}
if(!is.numeric(points[,2]) | !is.numeric(points[,3])) {
stop("Data in columns two and three are non-numeric. Make sure they contain latitude and longitude values.")
}
cat("Start CoordinateCleaner filtering...")
# removing centroids
species <- colnames(points)[1]
latitude <- colnames(points)[2]
longitude <- colnames(points)[3]
points <- CoordinateCleaner::cc_cen(points, lon=longitude, lat=latitude, species = species, geod=T, buffer=ccbuffer, verbose=F)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_cap(points, lon=longitude, lat=latitude, species = species)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_dupl(points, lon=longitude, lat=latitude, species = species)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_equ(points, lon=longitude, lat=latitude)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_inst(points, lon=longitude, lat=latitude, species = species)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_val(points, lon=longitude, lat=latitude)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_sea(points, lon=longitude, lat=latitude)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
points <- CoordinateCleaner::cc_outl(points, lon=longitude, lat=latitude)
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
# removing coordinates with no decimal cases
cat("Removing coordinates with no decimal cases...")
length_decimal_lat <- nchar(sub("^[^.]*", "", points[,2]))-1
length_decimal_lon <- nchar(sub("^[^.]*", "", points[,3]))-1
points <- points[which(length_decimal_lat>=0 & length_decimal_lon>=0),]
if (nrow(points)==0) {
cat("No points remained after cleaning.")
return(points)
}
cat("Cleaning complete.")
return(points)
}
Thinning <- function (points, distance = 50, verbose) {
species <- colnames(points)[1]
latitude <- colnames(points)[2]
longitude <- colnames(points)[3]
if (nrow(points) > 3) {
suppressWarnings(spThin::thin(loc.data=points, lat.col=latitude, long.col=longitude, spec.col=species,
thin.par=distance, reps=10, write.files = T, log.file="temp.txt", out.dir=getwd(),
out.base=species, max.files=1, verbose=FALSE))
thinfile <- list.files(path=getwd(), pattern="thin1.csv")
points <- read.csv(paste(getwd(),thinfile, sep="/"), stringsAsFactors = F)
unlink(paste(getwd(),thinfile, sep="/"))
unlink(paste0(getwd(),"/","temp.txt"))
points <- points[,c(1,3,2)] # reorganizing lat and lon order
return(points)
} else {
if(verbose) {
cat("Not enough points for thinning... returning original dataset.") }
return(points)
}
}
LoadWcLayers <- function (res.layers) {
bio <- raster::getData("worldclim", var="bio", res=res.layers, path=getwd()) # climatic layers
return(bio)
}
BgPolygon <- function (thinned_points, buffer.polygon=c(2,5), overwrite.dir=TRUE, plot.polygons=FALSE) {
max.lat <- ceiling(max(thinned_points[,2])) + buffer.polygon[1]
min.lat <- floor(min( thinned_points[,2])) - buffer.polygon[1]
max.lon <- ceiling(max(thinned_points[,3])) + buffer.polygon[2]
min.lon <- floor(min(thinned_points[,3])) - buffer.polygon[2]
if(max.lat > min.lat + 50 & max.lon > min.lon + 100){
warning("Very wide background detected - this may not correspond to a natural distribution.")
}
bg.area <- raster::extent(x = c(min.lon, max.lon, min.lat, max.lat))
if(plot.polygons) {
polygon.dir <- paste0(getwd(),"/polygon")
if (overwrite.dir) {
unlink(polygon.dir, recursive=TRUE) # overwrites output directory for models
dir.create(file.path(polygon.dir))
} else { suppressWarnings(dir.create(polygon.dir)) }
pdf(file=paste0(polygon.dir,"/", thinned_points[1,1],"__background_polygon.pdf"), height=6, width=10)
plot(wrld_simpl)
plot(bg.area, col="orange", add=T)
title(main=paste0(thinned_points[1,1]))
dev.off()
}
return(bg.area)
}
ColinearityTest <- function(bg, predictors) {
layers <- raster::crop(predictors, raster::extent(bg))
v0 <- suppressWarnings(usdm::vifcor(layers, th=0.8)) # stablished threshold
predictors_final <- usdm::exclude(layers, v0) # excludes variables that have collinearity problems
predictors_final <- raster::stack(predictors_final)
return(predictors_final)
}
RangeFromSDM <- function (thinned_points, predictors_final, method, overwrite.modeldir=TRUE) {
model.dir <- paste0(getwd(),"/ranges")
if (overwrite.modeldir) {
unlink(model.dir, recursive=TRUE) # overwrites output directory for models
dir.create(file.path(model.dir))
} else { suppressWarnings(dir.create(model.dir)) }
points_for_sdm = thinned_points
species_name <- as.character(points_for_sdm[1,1])
points_for_sdm[,1] <- 1
sp::coordinates(points_for_sdm) <- ~ lon + lat
# modeling
d <- sdm::sdmData(species~., points_for_sdm, predictors = predictors_final, bg = list(n = 1000)) # 1000 background points
m <- sdm::sdm(species~., d, methods = method, replication = "cv", cv.folds = 5, test.percent = 30)
auc0 <- mean(sdm::getEvaluation(m, w = 1:5, stat = "AUC")$AUC)
model <- ensemble(m, predictors_final, setting=list(method = 'weighted',stat = "AUC"), file = paste0(model.dir,"/", species_name))
results <- list()
results[[1]] <- paste0(species_name)
results[[2]] <- model
results[[3]] <- paste(method)
results[[4]] <- auc0
names(results) <- c("species_name","original_model", "sdm_method", "mean_auc")
return(results)
}
RangeSize <- function (list_of_model_results, threshold) {
#wrld_simpl <- readRDS("R/wrld_simpl.Rdata")
model <- list_of_model_results$original_model
model[model[] < threshold] <- NA
model[!is.na(model)] <- 1
range_size <- round(sum(area(model, na.rm=T)[], na.rm=T), 2)
writeRaster(model, filename=paste0(getwd(),"/ranges/", list_of_model_results$species_name, "_range.tif"), overwrite=T)
list_of_model_results[[5]] <- model
list_of_model_results[[6]] <- paste(threshold)
list_of_model_results[[7]] <- range_size
names(list_of_model_results)[c(5,6,7)] <- c("range", "threshold","range_size")
return(list_of_model_results)
}
AddAlerts <- function(fullresults, bg) {
alerts <- c()
# cross check with list of crops and invasive species
crops <- readRDS("crops_taxized_gbif.Rdata")
invasive <- readRDS("invasive_taxized_gbif.Rdata")
if(fullresults$species_name %in% crops) {
crop_alert <- "Crop alert: species may be a crop and its range may be non-natural. Check xxx (website) for more information."
alerts <- c(alerts, crop_alert)
}
if(fullresults$species_name %in% invasive) {
invasive_alert <- "Invasive alert: species may be invasive and its range may be non-natural. Check xxx (website) for more information."
alerts <- c(alerts, invasive_alert)
}
# is AUC too low?
if(fullresults$mean_auc < 0.75) {
low_auc_alert <- "Alert of low AUC: model has AUC lower than 0.75."
alerts <- c(alerts, low_auc_alert)
}
# is range too wide? (proxy for non-natural distribution)
lims <- bg[]
if(lims[2] > lims[1] + 100 & lims[4] > lims[3] + 50) {
wide_range_alert <- "Alert of wide range: range seems too wide for a natural distribution. Check quality of distribution data."
alerts <- c(alerts, wide_range_alert)
} else if(fullresults$range_size > 10000000) {
wide_range_alert <- "Alert of wide range: range seems too wide for a natural distribution. Check quality of distribution data."
alerts <- c(alerts, wide_range_alert)
}
# is number of points use in model below three? (data deficiency alert)
if(is.null(alerts)) {
fullresults[[8]] <- "No alerts returned."
names(fullresults)[8] <- "alerts"
} else {
fullresults[[8]] <- alerts
names(fullresults)[8] <- "alerts"
}
return(fullresults)
}
WcFromRange <- function (range_raster, res=10, layer=1, plot=TRUE) {
bio <- raster::getData("worldclim", var="bio", res=res)
cat("Extracting climate information...",  "\n")
results_tmp1 <- list()
results_tmp1[[1]] <- paste0("bio_", layer)
if(plot) {
results_tmp2 <- boxplot(raster::extract(bio[[layer]],  rasterToPolygons(range_raster, dissolve=TRUE))[[1]], plot=TRUE, ylab=paste0("bio_", layer))
results <- c(results_tmp1, results_tmp2)
names(results)[1] <- "Woldclim layer"
} else {
results_tmp2 <- boxplot(raster::extract(bio[[layer]],  rasterToPolygons(range_raster, dissolve=TRUE))[[1]], plot=FALSE)
results <- c(results_tmp1, results_tmp2)
names(results)[1] <- "Woldclim layer"
}
return(results)
}
GetRange <- function(points, method="maxent", threshold = 0.9) {
cat("Thinning points...")
thinned_points <- Thinning(points)
cat("Loading environmental predictors...")
predictors <- LoadWcLayers(res.layers=10)
cat("Creating background polygon...")
bg <- BgPolygon(thinned_points)
cat("Removing predictors that have colinearity problems ...")
predictors_final <- ColinearityTest(bg, predictors)
cat("Performing species distribution modeling...")
list_of_model_results <- RangeFromSDM(thinned_points, predictors_final, method)
cat("Making models binary based on threshold...")
fullresults <- RangeSize(list_of_model_results, threshold)
cat("Adding alerts...")
fullresults_w_alerts <- AddAlerts(fullresults, bg)
cat("Done.")
return(fullresults_w_alerts)
}
AltitudeFromRange <- function (range_raster, res=10, layer=1, plot=TRUE) {
bio <- raster::getData("worldclim", var="alt", res=res)
cat("Extracting climate information...",  "\n")
results_tmp1 <- list()
results_tmp1[[1]] <- paste0("bio_", layer)
if(plot) {
results_tmp2 <- boxplot(raster::extract(bio[[layer]],  rasterToPolygons(range_raster, dissolve=TRUE))[[1]], plot=TRUE, ylab=paste0("bio_", layer))
results <- c(results_tmp1, results_tmp2)
names(results)[1] <- "Woldclim layer"
} else {
results_tmp2 <- boxplot(raster::extract(bio[[layer]],  rasterToPolygons(range_raster, dissolve=TRUE))[[1]], plot=FALSE)
results <- c(results_tmp1, results_tmp2)
names(results)[1] <- "Woldclim layer"
}
return(results)
}
setwd("./data")
library(sdm)
library(raster)
library(CoordinateCleaner)
library(spThin)
library(usdm)
library(dismo)
library(maptools)
library(rJava)
data("wrld_simpl")
GetRange <- function(points, method="maxlike", threshold = 0.9) {
cat("Thinning points...")
thinned_points <- Thinning(points)
cat("Loading environmental predictors...")
predictors <- LoadWcLayers(res.layers=10)
cat("Creating background polygon...")
bg <- BgPolygon(thinned_points)
cat("Removing predictors that have colinearity problems ...")
predictors_final <- ColinearityTest(bg, predictors)
cat("Performing species distribution modeling...")
list_of_model_results <- RangeFromSDM(thinned_points, predictors_final, method)
cat("Making models binary based on threshold...")
fullresults <- RangeSize(list_of_model_results, threshold)
cat("Adding alerts...")
fullresults_w_alerts <- AddAlerts(fullresults, bg)
cat("Done.")
return(fullresults_w_alerts)
}
pleroma <- QuickGBIFSearch("Svitramia hatschbachii") # baixa os pontos
