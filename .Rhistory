basedir = "~/Desktop/MiSSEgradient/MiSSEGradient/full_run"
taxonomy_tree = phy.list(input.dir=paste0(basedir,"/Data"), names="ALLMB.taxized.tre")[[1]]
Resolve <- function(phy, prune_unmatched=TRUE, delete_author=FALSE) {
sources <- taxize::gnr_datasources()
gbif <- sources$id[sources$title == 'GBIF Backbone Taxonomy']
gnr_resolve_x <- function(x, data_source_ids=gbif, best_match_only = TRUE) {
new.name <- suppressWarnings(taxize::gnr_resolve(names=x, data_source_ids=data_source_ids, best_match_only=best_match_only)$matched_name)
if(is.null(new.name)) {
new.name <- paste0("UNMATCHED_",x)
}
return(new.name)
}
new.names <- pbapply::pbsapply(phy$tip.label, gnr_resolve_x)
if(length(new.names) != length(phy$tip.label)) {
stop("Fewer names returned")
}
phy$tip.label <- new.names
if(prune_unmatched) {
bad.tips <- phy$tip.label[grepl("UNMATCHED_", phy$tip.label)]
if(length(bad.tips)>0) {
phy <- ape::drop.tip(phy, bad.tips)
}
}
if(delete_author) {
tip_name <- strsplit(phy$tip.label," ",fixed = TRUE)
phy$tip.label <- paste(sapply(tip_name, "[", 1), sapply(tip_name, "[", 2), sep="_")
}
return(phy)
}
GetSubTree <- function(clade.name, full_phy){
node.number <- which(full_phy$node.label == clade.name) + Ntip(full_phy)
all.tips <- Descendants(full_phy, node.number, type = c("tips"))[[1]]
set <- full_phy$tip.label
rows <- which(set %in% full_phy$tip.label[all.tips])
byebye <- set[-rows]
subTree <- drop.tip(full_phy, as.character(byebye))
return(subTree)
}
CleanGBIFRecords <- function(gbif.file, basedir) {
#Basic idea, is to do everything as we search by taxon:
subset.mat <- gbif.file
#Step 1: Parse out duplicates with respect to lat and long:
subset.mat <- subset.mat[!duplicated(subset.mat[,3]) & !duplicated(subset.mat[,4]),]
#Step 2: Remove human observations from the list.
subset.mat <- subset.mat[which(!subset.mat[,5]=="HUMAN_OBSERVATION" & !subset.mat[,5]=="UNKNOWN" & !subset.mat[,5]=="LIVING_SPECIMEN"),]
#Step 3: Remove centroids
subset.mat <- CoordinateCleaner::cc_cen(subset.mat, lon="long", lat="lat", species ="name", geod=T, buffer=25000, verbose=F)
#Step 4: Remove points with no decimal cases
length_decimal_lat <- nchar(sub("^[^.]*", "", subset.mat$lat))-1
length_decimal_lon <- nchar(sub("^[^.]*", "", subset.mat$long))-1
subset.mat <- subset.mat[which(length_decimal_lat>=2 & length_decimal_lon>=2),]
#Step 5: Removing outliers in distribution
unique(subset.mat$name)->species
cleaned_point <- data.frame()
for(cleaning in 1:length(species)){
sp0<-subset.mat[subset.mat$name==species[cleaning],]
out_lat<-boxplot.stats(sp0$lat)$out
out_lon<-boxplot.stats(sp0$long)$out
sp0 <- sp0[ ! sp0$lat %in% out_lat, ]
sp0 <- sp0[ ! sp0$long %in% out_lon, ]
cleaned_point <- rbind(cleaned_point, sp0)
}
subset.mat<-cleaned_point
#Step 6: Remove points that are likely the location of herbaria
#Basically, I first match by lat, then by long.
# herbaria_localities <- read.csv("/home/tvasconcelos/MiSSEGradient/Data/allHerbaria_ADM1_badCoords.txt", header=FALSE)
herbaria_localities <- read.csv(paste0(basedir, "/Data/allHerbaria_ADM1_badCoords.txt"), header=FALSE)
for(herb.index in 1:length(herbaria_localities)){
subset.mat <- subset.mat[which(!round(subset.mat$lat,2)==herbaria_localities[herb.index,1] | !round(subset.mat$long,2)==herbaria_localities[herb.index,2]),]
}
#Step 7: Remove points based on issues. Removed any records with the following codes: "cdout", "cdrepf", etc.
tmp.vector <- c()
if(dim(subset.mat)[1]>0){
issues.list.by.record <- strsplit(as.character(subset.mat$issues), ",")
for(record.index in 1:dim(subset.mat)[1]){
if(any(issues.list.by.record[[record.index]] == "ccm" | issues.list.by.record[[record.index]] == "conti" | issues.list.by.record[[record.index]] == "cdiv" | issues.list.by.record[[record.index]] == "cdout" | issues.list.by.record[[record.index]] == "cdrepf" | issues.list.by.record[[record.index]] == "cdreps" | issues.list.by.record[[record.index]] == "cucdmis" | issues.list.by.record[[record.index]] == "cuiv" | issues.list.by.record[[record.index]] == "cum" |  issues.list.by.record[[record.index]] == "gdativ" | issues.list.by.record[[record.index]] == "reneglat" | issues.list.by.record[[record.index]] == "reneglon" | issues.list.by.record[[record.index]] == "preswcd" | issues.list.by.record[[record.index]] == "zerocd" | issues.list.by.record[[record.index]] == "txmatfuz" | issues.list.by.record[[record.index]] == "txmathi" | issues.list.by.record[[record.index]] == "txmatnon")){
tmp.vector <- tmp.vector
}else{
tmp.vector <- c(tmp.vector, record.index)
}
}
final.mat <- subset.mat[tmp.vector,]
return(final.mat)
}else{
return(NULL)
}
}
GetGbifRecordOneTaxonSurviveFailure <- function(taxon, clade="none", basedir=getwd()){
final.df <- c()
return.taxon <- NA
search.hits <- NA
slow.time <- 60
while(is.na(search.hits)[1]) {
try(search.hits <- occ_search(scientificName = taxon, limit=50000,  hasCoordinate=TRUE)$data)
if(is.null(search.hits)) { return("no_data") }
else if(is.na(search.hits)) {
Sys.sleep(slow.time)
slow.time <- min(slow.time*1.2, 60*15)
}
}
if((search.hits[1] == "no data found, try a different search")[1]){
print("No records")
}else{
tmp.df <- data.frame(name=search.hits$acceptedScientificName, key=search.hits$key, lat=search.hits$decimalLatitude, long=search.hits$decimalLongitude, basisOfRecord=search.hits$basisOfRecord, issues=search.hits$issues)
tmp.df.clean <- try(CleanGBIFRecords(tmp.df, basedir))
if(!is.null(tmp.df.clean) & class(tmp.df.clean)!="try-error") {
#We require that each species has, at a minimum, 10 records:
if(dim(tmp.df.clean)[1]>2){
names(tmp.df.clean) <- c("species", "key", "lat", "long", "basisOfRecord", "issue")
write.table(tmp.df.clean, file=paste0(basedir, "/Data/2_gbif/", clade, "__", gsub(" ", "_", taxon), ".csv"), quote=FALSE, sep="\t", row.names=FALSE, col.names=FALSE, append=FALSE)
return.taxon <- taxon
}
}
}
return(return.taxon)
}
DoOneClade <- function(individual_clade, large_gb_tree, ncores=1, basedir=getwd()) {
subtree <- GetSubTree(unname(individual_clade), large_gb_tree)
ape::write.tree(subtree, file=paste0(basedir,"/Data/1_trees/", names(individual_clade),".tre"))
taxized <- Resolve(subtree)
ape::write.tree(taxized, file=paste0(basedir,"/Data/1_trees/", names(individual_clade), ".taxized.tre"))
gbif_cleaned <- lapply(taxized$tip.label, FUN=GetGbifRecordOneTaxonSurviveFailure, clade=names(individual_clade),  basedir=basedir)
gbif_cleaned <- gbif_cleaned[!is.na(gbif_cleaned)]
gbif_cleaned_prefix <- paste0(paste0(names(individual_clade), "__"), gbif_cleaned)
return(gbif_cleaned_prefix)
}
# Functions to run closesamples #------------
phy.list <- function (input.dir=input.dir, names=names, search.for=".taxized.tre") {
l0 <- list.files(input.dir, search.for)
l1 <- sub(search.for, "", l0)
tree_names <- l0[which(l1 %in% names)]
# Read tree files and .txt
tmp.files1 <- lapply(paste0(input.dir,"/",tree_names), readLines)
# Removing single quotes from authority names
tmp.files2 <- lapply(tmp.files1, function(x) gsub("\\'","",as.character(x)))
tree_list <- list()
for(tree_index in sequence(length(tmp.files2))){
# Read tree files and phylo
t0 <- ape::read.tree(text=tmp.files2[[tree_index]])
t0 <- ape::ladderize(t0)
# Keeping only species names. Note that this will also lump "var." and "subsp." into the same species name.
name <- strsplit(t0$tip.label,"_",fixed = TRUE)
t0$tip.label <- paste(sapply(name, "[", 1), sapply(name, "[", 2), sep=" ")
t0$tip.label <- sub(",","", t0$tip.label)
# Removing node labels
t0$node.label <- NULL
# Removing duplicated tips and tips identified only to genus level
t0 <- ape::drop.tip(t0, which(unlist(lapply(1:length(name), function(x) stringr::str_detect(substr(name[[x]][2], 1, 1), "^[:upper:]+$")))))
t0 <- ape::drop.tip(t0, which(duplicated(t0$tip.label)))
tree_list[[tree_index]] <- t0
names(tree_list)[tree_index] <- sub(search.for, "", tree_names)[tree_index]
}
return(tree_list)
}
run.closesamples <- function(cladenames, phy_full=taxonomy_tree, fraction=0.75, input.dir=input.dir, ncores=4) {
clades0 <- phy.list(input.dir=input.dir, names=unname(cladenames), search.for=".taxized.tre")
clades <- clades0[[1]]
if(ape::Ntip(clades)-1 != ape::Nnode(clades)) { # removing all tips descending from polytomies
d0 <- tabulate(clades$edge[, 1])
polynodes <- which(d0 > 2)
polytips <- phangorn::Descendants(clades, polynodes, type="tips")
clades <- ape::drop.tip(clades, unique(clades$tip.label[unlist(polytips)]))
}
sub0 <-  closesamples::SubsampleTree(phy_feasible=clades, n = round(Ntip(clades)*fraction, 0), phy_full=phy_full, replace_full=FALSE, replace_feasible=FALSE, less_memory=TRUE, truncate_full_to_mrca=TRUE, fast_ultrametric=FALSE, verbose=FALSE)
ape::write.tree(sub0, file=paste0(input.dir, "/sampled_trees/", names(clades0), "_", format(Sys.time(), "%b%d%H%M%S"), sample(1:100, 1),".sampled.tre"))
return(sub0)
}
# Functions to run MiSSE #------------
# closesamples
MiSSEGreedy_fullrun_cs <- function(tree_files = tree_files, basedir, total_diversity=total_diversity) {
tree_file = unname(tree_files)
phy = ape::read.tree(paste0(basedir, "Data/sampled_trees/", tree_file))
tree_name = sapply(strsplit(tree_file, "_"), "[", 1)
total_clade = as.numeric(total_diversity[which(names(total_diversity) == tree_name)])
### parameters
turnover.tries = sequence(10)
eps.tries = sequence(3)
max.param = length(turnover.tries) + length(eps.tries)
# creating a matrix of possible combinations of parameter:
comb_1 = hisse::generateMiSSEGreedyCombinations(max.param = max.param, turnover.tries = turnover.tries,
eps.tries = eps.tries, fixed.eps.tries=c(0, 0.9, NA),
vary.both=T)
# run MiSSE models:
unique_name <- paste0(format(Sys.time(), "%b%d%H%M%S"), sample(1:100, 1))
run_1 = hisse::MiSSEGreedy(phy, f = ape::Ntip(phy)/total_clade, possible.combos = comb_1, save.file=paste0(basedir,"Data/modelfits/", tree_name,"_",unique_name,"_fits_run_1.Rsave"),
root.type="madfitz", stop.deltaAICc=Inf, chunk.size = 10, turnover.upper=100, trans.upper=10, sann=TRUE, sann.its=1000)
model.recons1 <- as.list(1:length(run_1))
for (model_index in 1:length(run_1)) {
nturnover <- length(unique(run_1[[model_index]]$turnover))
neps <- length(unique(run_1[[model_index]]$eps))
hisse_recon <- hisse::MarginReconMiSSE(phy = run_1[[model_index]]$phy, f = Ntip(phy)/total_clade, hidden.states = nturnover,
pars = run_1[[model_index]]$solution, aic = run_1[[model_index]]$AIC, root.type = "madfitz")
model.recons1[[model_index]] <- hisse_recon
}
save(model.recons1, file = paste0(basedir,"Data/modelrecons/", tree_name, "_", unique_name ,"_model_recons1.Rsave"))
return(model.recons1)
}
taxonomy_tree = phy.list(input.dir=paste0(basedir,"/Data"), names="ALLMB.taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/1_trees"), names="ALLMB", search.for=".taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/"), names="ALLMB", search.for=".taxized.tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data/"), names="GBMB", search.for=".tre")[[1]]
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for="taxized.tre")[[1]]
basedir
input.dir=paste0(basedir, "/Data")
input.dir
phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for="taxized.tre")
taxonomy_tree = phy.list(input.dir=paste0(basedir, "/Data"), names="ALLMB", search.for=".taxized.tre")
rmarkdown::render_site()
rmarkdown::render_site()
